1. Introdução à análise de complexidade.
2. A importância da análise de complexidade para entender estruturas de dados.
3. A análise de complexidade como base para problemas de entrevistas de codificação.
4. Discussão sobre como um único problema de codificação pode ter várias soluções.
5. Explicação de que nem todas as soluções são iguais - algumas são melhores que outras.
6. Discussão sobre a pergunta comum em entrevistas de codificação: "Você pode fazer melhor?".
7. Explicação de que a "melhor" solução é aquela com melhor complexidade.
8. Definição de complexidade em termos de ciência da computação, não em termos de dificuldade.
9. Introdução aos conceitos de complexidade de tempo e complexidade de espaço.
10. Explicação de que a complexidade de tempo é uma medida de quão rápido um algoritmo ou solução funciona.
11. Explicação de que a complexidade de espaço é uma medida de quanto memória um algoritmo usa.
12. Discussão sobre como a melhor solução é aquela com melhor complexidade de tempo ou espaço.
13. Explicação de que a análise de complexidade é o processo de descobrir a complexidade de um algoritmo ou solução.
14. Ligação da análise de complexidade de volta às estruturas de dados.
15. Revisão da definição de estruturas de dados: conjuntos de valores de dados, relações entre esses valores e operações que podem ser aplicadas a esses valores.
16. Explicação de como as operações em estruturas de dados têm implicações de complexidade de tempo e espaço.
17. Discussão sobre como diferentes estruturas de dados têm diferentes implicações de complexidade de tempo e espaço para as funções e operações que suportam.
18. A importância de escolher a estrutura de dados que não só resolve o problema, mas também o faz com a melhor complexidade de tempo ou espaço.
19. A necessidade de familiaridade com as implicações de complexidade de tempo-espaço das várias estruturas de dados populares.
20. Introdução ao conceito de memória como outra camada de conhecimento fundamental para entender a análise de complexidade e, por sua vez, as estruturas de dados.
21. Discussão sobre como a compreensão da memória pode ajudar a entender como o código funciona "sob o capô".
22. Aviso de que a memória é um tópico amplo e que este vídeo simplificará muitas coisas para torná-lo relevante para entrevistas de codificação.
23. Explicação de como a declaração e o uso de variáveis em código envolvem o armazenamento dessas variáveis em algum lugar, que é onde a memória entra em jogo.
24. Descrição de memória como um "quadro" limitado dividido em vários slots.
25. Discussão sobre a importância de um algoritmo usar menos memória devido à natureza limitada da memória.
26. Explicação de como um programa armazena uma variável, como um número, em um slot de memória.
27. Nota de que um programa sempre armazenará uma variável em um slot de memória livre.
28. Discussão sobre como um programa armazenará uma variável que precisa de mais de um slot de memória em slots de memória consecutivos.
29. Explicação de como a memória é feita de bits, que são zeros e uns, e que um slot de memória pode conter oito bits.
30. Explicação de que um slot de memória pode conter oito bits, que é chamado de byte.
31. Discussão sobre como as informações são armazenadas em zeros e uns, ou em formato binário.
32. Explicação de que qualquer pedaço de dados pode ser transformado em formato binário e armazenado em blocos de oito bits, ou bytes.
33. Discussão sobre como um byte, que é oito bits, só dá 2^8, ou 256, valores de dados potenciais que podem ser representados.
34. Explicação de que para representar mais valores de dados, aumentaríamos o número de bits que representam esse valor de dados.
35. Discussão sobre como muitas arquiteturas de computadores ou linguagens de programação representam inteiros em formato de 32 bits ou 64 bits.
36. Explicação de que um número, como o número um, que é representado como um inteiro de 32 bits, ocuparia quatro bytes, o que significa que o programa precisaria armazená-lo em quatro slots de memória consecutivos.
37. Explicação de que a representação de um número em binário é armazenada em bytes, que são armazenados em slots de memória.
38. Discussão sobre o conceito de endianness, que tem a ver com a ordem dos bytes ao representar um número em binário.
39. Explicação de que estamos normalmente lidando com inteiros de largura fixa, o que significa que um inteiro de 32 bits sempre ocupará quatro slots de memória, e um inteiro de 64 bits sempre ocupará oito slots de memória.
40. Discussão sobre como um programa encontra slots de memória consecutivos livres para armazenar um número.
41. Explicação de como uma lista de números, como [1, 2], seria armazenada em memória, com cada número ocupando seus próprios slots de memória consecutivos.
42. Discussão sobre como strings são armazenadas em memória, com cada caractere mapeado para um número através de um mapeamento como ASCII, e então esse número é transformado em bits.
43. Introdução ao conceito de ponteiros, que permitem armazenar o endereço de memória de outro slot de memória em um slot de memória.
44. Explicação de que os ponteiros podem ser úteis porque permitem apontar para um slot de memória que pode estar muito distante, sem a necessidade de slots de memória consecutivos.
45. Nota de que o programa ou o computador pode acessar qualquer slot de memória diretamente e muito rapidamente.
46. Recapitulação de que a memória é um conjunto limitado de slots que podem armazenar dados, e que é possível esgotar a memória, o que é por que nos importamos com a complexidade do espaço.
47. Reafirmação de que a informação é armazenada em slots de memória em formato binário como bits, e que um slot de memória pode conter oito bits, que é chamado de byte.
48. Discussão sobre como, quando um inteiro é armazenado na memória, ele é normalmente um inteiro de largura fixa, o que significa que sabemos quantos bits ele terá e quantos slots de memória ele ocupará.
49. Explicação de que qualquer tipo de informação pode ser transformada em formato binário e, portanto, transformada em bits que podem ser armazenados na memória.
50. Discussão sobre como acessar slots de memória é uma operação básica e muito rápida.
51. Introdução ao próximo tópico: o logaritmo e sua importância na análise de complexidade.
52. Explicação da definição matemática do logaritmo, onde o logaritmo de x na base b é igual a y se e somente se b elevado à potência y é igual a x.
53. Nota de que, em ciência da computação e em entrevistas de codificação, sempre assumimos que a base do logaritmo é dois, o que é chamado de logaritmo binário.
54. Explicação de que o logaritmo de N é a potência à qual você precisa elevar dois para obter N.
55. Exemplos de cálculos de logaritmos, como log de 1 é 0, log de 4 é 2, e log de 16 é 4.
57. Discussão sobre como, quando você dobra o número N, você está apenas aumentando o logaritmo de N por um.
58. Exemplos de cálculos de logaritmos, como log de 32 é 5 e log de 16 é 4, mostrando que dobrar N aumenta o logaritmo de N por um.
59. Explicação de que, à medida que N aumenta, o logaritmo de N aumenta apenas uma pequena quantidade.
60. Discussão sobre como, quando a complexidade do tempo de um algoritmo é logarítmica, isso é muito bom, pois significa que, à medida que a entrada dobra, o número de operações elementares que o algoritmo realiza aumenta apenas por um.
61. Comparação da complexidade logarítmica com a complexidade linear, mostrando que a complexidade logarítmica é muito melhor à medida que o tamanho da entrada aumenta.
62. Descrição de um gráfico de complexidade, mostrando como a complexidade linear aumenta linearmente com N, enquanto a complexidade logarítmica aumenta modestamente no início e depois se nivela.
63. Discussão de um exemplo de um algoritmo com complexidade logarítmica, onde um array de comprimento N é cortado pela metade a cada passo.
64. Explicação de que, à medida que o array é cortado pela metade a cada passo, o número total de operações realizadas pelo algoritmo é logarítmico em relação ao comprimento do array.
65. Nota de que, quando o comprimento do array é dobrado, apenas uma operação extra é realizada, o que significa que a complexidade do tempo do algoritmo é logarítmica.
66. Discussão de outro exemplo de um algoritmo com complexidade logarítmica, onde uma árvore binária é cortada pela metade a cada passo.
67. Explicação de que, à medida que a árvore binária é cortada pela metade a cada passo, o número total de operações realizadas pelo algoritmo é logarítmico em relação ao tamanho da árvore.
68. Discussão sobre como um algoritmo com complexidade logarítmica pode ser identificado perguntando se o algoritmo está eliminando metade da entrada a cada passo.
69. Explicação de que, se a resposta for sim, então o algoritmo provavelmente tem uma complexidade logarítmica, assumindo que não está realizando operações auxiliares a cada passo.
70. Nota de que a mesma lógica se aplica à complexidade do espaço.
71. Discussão sobre como, se o tamanho da entrada for dobrado e apenas uma operação extra for realizada, então o algoritmo provavelmente tem uma complexidade logarítmica.
72. Discussão sobre como a complexidade logarítmica se aplica a árvores binárias, onde metade da árvore é eliminada a cada passo.
73. Explicação de que, se o tamanho da árvore for dobrado e apenas uma operação extra for realizada, então o algoritmo provavelmente tem uma complexidade logarítmica.
74. Introdução ao próximo tópico: Arrays.
75. Discussão sobre como um array é armazenado na memória, com cada elemento do array ocupando um número fixo de slots de memória.
76. Explicação de que o sistema operacional precisa encontrar slots de memória suficientes para armazenar o array, o que significa que o número de slots de memória usados é fixo e está diretamente ligado ao tamanho do array.
77. Introdução aos conceitos de arrays estáticos e dinâmicos.
78. Discussão sobre como, em algumas linguagens de programação como C++ e Java, é necessário especificar o tamanho do array ao declará-lo, o que é chamado de array estático.
79. Explicação de que, em um array estático, o sistema operacional sabe exatamente quantos slots de memória precisa alocar para o array.
80. Nota de que, se o tamanho do array não fosse especificado, o sistema operacional não saberia quantos slots de memória alocar.
81. Discussão sobre como a operação mais comum em um array é ler um valor em um determinado índice.
82. Explicação de que a leitura de um valor em um determinado índice é uma operação básica que é realizada quase instantaneamente, o que significa que ela é realizada em tempo constante.
83. Discussão sobre como um array é acessado na memória, com o sistema operacional encontrando o endereço de memória que inicia o array e então determinando quantos slots de memória são necessários para cada elemento do array.
84. Explicação de que o sistema operacional sabe quantos slots de memória são necessários para cada elemento do array porque estamos sempre lidando com inteiros de 64 bits, que ocupam oito slots de memória.
85. Discussão sobre como o sistema operacional pode determinar o endereço de memória de um elemento em um array multiplicando o índice do elemento pelo número de slots de memória necessários para cada elemento e adicionando o resultado ao endereço de memória que inicia o array.
86. Nota de que acessar um elemento em um array é uma operação de tempo constante, pois envolve apenas uma multiplicação e uma adição, ambas operações elementares que são realizadas quase instantaneamente.
87. Discussão sobre como a operação de definir um elemento em um array é semelhante à operação de acessar um elemento em um array, com o sistema operacional encontrando o endereço de memória do elemento e então substituindo os bits nesse endereço de memória.
88. Nota de que a operação de definir um elemento em um array também é uma operação de tempo constante, pois envolve apenas uma multiplicação, uma adição e uma substituição de bits, todas operações elementares que são realizadas quase instantaneamente.
89. Discussão sobre como a operação de inicializar um array envolve informar ao sistema operacional o tamanho do array e o sistema operacional alocando a quantidade necessária de slots de memória para o array.
90. Discussão sobre como um array é inicializado na memória, com o sistema operacional encontrando slots de memória suficientes para armazenar o array, o que significa que o número de slots de memória usados é fixo e está diretamente ligado ao tamanho do array.
91. Explicação de que a inicialização de um array é uma operação de tempo linear e de espaço linear, pois envolve a alocação de uma quantidade de slots de memória que é diretamente proporcional ao tamanho do array.
92. Discussão sobre como a operação de percorrer um array é uma operação de tempo linear e de espaço constante, pois envolve a passagem por cada elemento do array uma vez, mas não requer a alocação de nenhum espaço de memória adicional.
93. Nota de que a operação de copiar um array é uma operação de tempo linear e de espaço linear, pois envolve a passagem por cada elemento do array uma vez e a alocação de uma quantidade de slots de memória que é diretamente proporcional ao tamanho do array.
94. Discussão sobre como várias operações comuns de array, como mapear, filtrar e reduzir, são operações de tempo linear, pois envolvem a passagem por cada elemento do array uma vez.
95. Discussão sobre como a operação de inserir um elemento em um array é uma operação de tempo linear e de espaço constante, pois envolve a passagem por cada elemento do array uma vez e a alocação de uma quantidade de slots de memória que é diretamente proporcional ao tamanho do array.
96. Explicação de que a operação de inserir um elemento em um array envolve informar ao sistema operacional o tamanho do array e o sistema operacional alocando a quantidade necessária de slots de memória para o array.
97. Discussão sobre como a operação de inserir um elemento em um array é uma operação de tempo linear e de espaço constante, pois envolve a passagem por cada elemento do array uma vez e a alocação de uma quantidade de slots de memória que é diretamente proporcional ao tamanho do array.
98. Introdução aos conceitos de arrays estáticos e dinâmicos.
99. Discussão sobre como, em algumas linguagens de programação como C++ e Java, é necessário especificar o tamanho do array ao declará-lo, o que é chamado de array estático.
100. Explicação de que, em um array estático, o sistema operacional sabe exatamente quantos slots de memória precisa alocar para o array.
101. Discussão sobre como um array dinâmico funciona, com o sistema operacional alocando o dobro do espaço de memória necessário para o array.
102. Explicação de que, quando um elemento é adicionado a um array dinâmico, o sistema operacional verifica se há espaço suficiente no array para o novo elemento e, se não houver, aloca o dobro do espaço de memória necessário para o array.
103. Nota de que, quando o espaço de memória alocado para um array dinâmico é esgotado, o sistema operacional aloca o dobro do espaço de memória necessário para o array e copia os elementos do array para o novo espaço de memória.
104. Discussão sobre como a operação de adicionar um elemento a um array dinâmico é uma operação de tempo constante se houver espaço suficiente no array para o novo elemento, mas é uma operação de tempo linear se o espaço de memória alocado para o array precisar ser dobrado.
105. Explicação de que a operação de adicionar um elemento a um array dinâmico é uma operação de espaço constante, pois não requer a alocação de nenhum espaço de memória adicional além do espaço de memória alocado para o array.
106. Discussão sobre como a complexidade do tempo da operação de adicionar um elemento a um array dinâmico é amortizada para O(1), pois a operação é uma operação de tempo constante na maioria das vezes, mas é uma operação de tempo linear quando o espaço de memória alocado para o array precisa ser dobrado.
107. Discussão sobre como a operação de inserir um elemento em um array dinâmico é uma operação de tempo linear e de espaço constante, pois envolve a passagem por cada elemento do array uma vez e a alocação de uma quantidade de slots de memória que é diretamente proporcional ao tamanho do array.
108. Explicação de que a operação de inserir um elemento em um array dinâmico envolve informar ao sistema operacional o tamanho do array e o sistema operacional alocando a quantidade necessária de slots de memória para o array.
109. Discussão sobre como a operação de inserir um elemento em um array dinâmico é uma operação de tempo linear e de espaço constante, pois envolve a passagem por cada elemento do array uma vez e a alocação de uma quantidade de slots de memória que é diretamente proporcional ao tamanho do array.
110. Introdução aos conceitos de arrays estáticos e dinâmicos.
111. Discussão sobre como, em algumas linguagens de programação como C++ e Java, é necessário especificar o tamanho do array ao declará-lo, o que é chamado de array estático.
112. Explicação de que, em um array estático, o sistema operacional sabe exatamente quantos slots de memória precisa alocar para o array.
113. Discussão sobre como a operação de inserir um elemento em um array dinâmico é uma operação de tempo linear e de espaço constante, pois envolve a passagem por cada elemento do array uma vez e a alocação de uma quantidade de slots de memória que é diretamente proporcional ao tamanho do array.
114. Explicação de que a operação de inserir um elemento em um array dinâmico envolve informar ao sistema operacional o tamanho do array e o sistema operacional alocando a quantidade necessária de slots de memória para o array.
115. Discussão sobre como a operação de inserir um elemento em um array dinâmico é uma operação de tempo linear e de espaço constante, pois envolve a passagem por cada elemento do array uma vez e a alocação de uma quantidade de slots de memória que é diretamente proporcional ao tamanho do array.
116. Introdução aos conceitos de arrays estáticos e dinâmicos.
117. Discussão sobre como, em algumas linguagens de programação como C++ e Java, é necessário especificar o tamanho do array ao declará-lo, o que é chamado de array estático.
118. Explicação de que, em um array estático, o sistema operacional sabe exatamente quantos slots de memória precisa alocar para o array.
119. Discussão sobre como a operação de inserir um elemento em um array dinâmico é uma operação de tempo linear e de espaço constante, pois envolve a passagem por cada elemento do array uma vez e a alocação de uma quantidade de slots de memória que é diretamente proporcional ao tamanho do array.
120. Explicação de que a operação de inserir um elemento em um array dinâmico envolve informar ao sistema operacional o tamanho do array e o sistema operacional alocando a quantidade necessária de slots de memória para o array.
121. Discussão sobre como a operação de inserir um elemento em um array dinâmico é uma operação de tempo linear e de espaço constante, pois envolve a passagem por cada elemento do array uma vez e a alocação de uma quantidade de slots de memória que é diretamente proporcional ao tamanho do array.
122. Introdução aos conceitos de arrays estáticos e dinâmicos.
123. Discussão sobre como, em algumas linguagens de programação como C++ e Java, é necessário especificar o tamanho do array ao declará-lo, o que é chamado de array estático.
124. Explicação de que, em um array estático, o sistema operacional sabe exatamente quantos slots de memória precisa alocar para o array.
125. Discussão sobre a mutabilidade das strings em diferentes linguagens de programação, com algumas linguagens como C++ permitindo a alteração de strings e outras linguagens como Python, Java, JavaScript, C# e Go tornando as strings imutáveis.
126. Explicação de que, em linguagens onde as strings são imutáveis, qualquer tentativa de alterar uma string resulta na criação de uma nova string.
127. Discussão sobre como a operação de adicionar um caractere a uma string em uma linguagem onde as strings são imutáveis é uma operação de tempo linear e de espaço constante, pois envolve a criação de uma nova string com o caractere adicionado ao final da string original.
127. Discussão sobre operações comuns com strings, como encontrar substrings dentro de uma string, com algoritmos rápidos, mas complexos, como o algoritmo Knuth-Morris-Pratt.
128. Explicação de que existem várias outras operações que se baseiam em coisas como encontrar uma substring, uma delas é substituir uma substring por outro valor em uma string.
129. Nota de que essas operações são frequentemente incorporadas à linguagem com a qual você está trabalhando.
130. Discussão sobre como, no contexto de entrevistas de codificação, as principais coisas que você precisa saber sobre strings são realmente as coisas fundamentais que você fará com elas.
131. Discussão sobre a importância de saber sobre a imutabilidade das strings se você estiver lidando com uma linguagem onde as strings são imutáveis.
132. Introdução ao próximo tópico: Pilhas e Filas.
133. Discussão sobre como as pilhas e filas são estruturas de dados interessantes que são comumente usadas em entrevistas de codificação.
134. Explicação de que pilhas e filas são fáceis de entender porque têm exemplos intuitivos da vida real e não são muito difíceis de implementar.
135. Discussão sobre o que é uma pilha e o que é uma fila, com exemplos da vida real, como uma pilha de livros em uma mesa para uma pilha e uma fila de pessoas esperando para comprar ingressos no cinema para uma fila.
136. Discussão sobre como as pilhas e filas suportam a inserção e remoção de elementos em tempo constante.
137. Explicação de que, em uma pilha, adicionar ou remover um livro pode ser feito em tempo constante, e em uma fila, você pode inserir e remover elementos em tempo constante.
138. Nota de que ambas as estruturas de dados suportam tempo constante e espaço constante para inserção e remoção de elementos.
139. Discussão sobre como uma pilha é realmente apenas um array dinâmico, onde adicionar um elemento ou remover um elemento do final do array são operações de tempo constante amortizado.
140. Nota de que a busca por um elemento arbitrário em uma pilha levará tempo linear, pois você teria que atravessar todo o array dinâmico.
141. Discussão sobre como uma fila é implementada usando uma lista vinculada, onde você mantém o controle tanto da cabeça quanto da cauda da lista vinculada.
142. Explicação de que, se você quiser adicionar um elemento à fila, você pode fazê-lo em tempo constante adicionando o elemento ao final da lista vinculada.
143. Nota de que, se você quiser remover um elemento da fila, você pode fazê-lo em tempo constante removendo o elemento do início da lista vinculada.
136. Explicação de que a operação de inserir um elemento em uma fila é chamada de "enqueue".
137. Discussão sobre como a operação de remover um elemento de uma fila é chamada de "dequeue".
138. Nota de que, para uma fila, você precisa ter uma referência para o final da lista vinculada, caso contrário, levaria tempo O(n) para encontrar o final.
139. Discussão sobre como as operações em uma pilha são tipicamente chamadas de "push" (para adicionar um elemento ao topo da pilha) e "pop" (para remover um elemento do topo da pilha).
140. Nota de que "push", "pop", "enqueue" e "dequeue" são sinônimos de inserir, deletar, adicionar e remover, respectivamente.
141. Discussão sobre como as filas são ligeiramente mais complexas de implementar do que as pilhas, o que é parcialmente por que às vezes você pode esquecer que uma fila é implementada com uma lista vinculada e apenas criar um array normal, que é super fácil de inicializar como uma fila.
142. Nota de que é importante informar ao entrevistador se você planeja usar um array normal como uma fila para economizar tempo na entrevista, mas ainda seria importante dizer a eles que você planeja usar uma fila que suporta o princípio de primeiro a entrar, primeiro a sair.
143. Discussão sobre como a inserção e a remoção em ambas as pilhas e filas ocorrem em tempo constante e espaço constante.
144. Discussão sobre como as pilhas e filas suportam um método de "peek", que também é executado em tempo constante e espaço constante. Este método é equivalente a "pop" e "dequeue", exceto que você não remove o elemento, apenas olha para o próximo elemento que seria removido.
145. Introdução ao próximo tópico: Tabelas Hash.
146. Discussão sobre como as tabelas hash são uma estrutura de dados extremamente poderosa e comum, usada tanto na prática quanto em entrevistas de codificação.
147. Nota de que provavelmente é preciso dizer que mais da metade das questões em entrevistas de codificação requerem algum tipo de tabela hash, então é muito importante saber como as tabelas hash funcionam.
148. Discussão sobre como usar tabelas hash é bastante simples na maioria das linguagens de programação, com muitas linguagens modernas tendo tabelas hash incorporadas que são diretamente suportadas.
149. Discussão sobre como uma tabela hash é uma loja de valores-chave, onde você é capaz de armazenar pares de chaves e valores onde cada chave mapeia para um valor.
150. Discussão sobre como a inserção de um novo par chave-valor, a exclusão de um par chave-valor ou a busca por um valor dado uma chave são todas operações que são executadas em tempo constante em média.
151. Nota de que isso pode parecer um array, no sentido de que em um array, temos pares de chaves e valores, exceto que as chaves são índices. As chaves são sempre números ou posições.
152. Discussão sobre como a coisa boa sobre tabelas hash é que nossas chaves não precisam ser números. Elas podem ser strings, e podem até ser outros tipos de dados.
153. Discussão sobre como uma tabela hash é implementada usando um array subjacente, com cada índice do array armazenando um valor que é mapeado a partir de uma chave.
154. Explicação de que uma função hash é usada para transformar uma chave em um índice que se encaixa no array subjacente.
155. Discussão sobre como a função hash transforma uma chave, como uma string, em um número inteiro, mapeando cada caractere na string para seu valor de codificação ASCII e somando todos esses valores.
156. Nota de que o número resultante da função hash é então modulado pelo comprimento do array subjacente para garantir que o índice resultante se encaixe no array.
157. Discussão sobre como a operação de inserir um novo par chave-valor em uma tabela hash envolve a transformação da chave em um índice usando a função hash e armazenando o valor no índice resultante no array subjacente.
158. Discussão sobre como a operação de buscar um valor dado uma chave em uma tabela hash envolve a transformação da chave em um índice usando a função hash e recuperando o valor no índice resultante no array subjacente.
159. Nota de que a operação de excluir um par chave-valor de uma tabela hash envolve a transformação da chave em um índice usando a função hash e removendo o valor no índice resultante no array subjacente.
160. Discussão sobre como a operação de inserir um novo par chave-valor, a operação de buscar um valor dado uma chave e a operação de excluir um par chave-valor de uma tabela hash são todas operações que são executadas em tempo constante em média.
161. Discussão sobre como uma tabela hash é implementada usando um array subjacente, com cada índice do array armazenando um valor que é mapeado a partir de uma chave.
162. Explicação de que uma função hash é usada para transformar uma chave em um índice que se encaixa no array subjacente.
163. Discussão sobre como a função hash transforma uma chave, como uma string, em um número inteiro, mapeando cada caractere na string para seu valor de codificação ASCII e somando todos esses valores.
164. Nota de que o número resultante da função hash é então modulado pelo comprimento do array subjacente para garantir que o índice resultante se encaixe no array.
165. Discussão sobre como a operação de inserir um novo par chave-valor em uma tabela hash envolve a transformação da chave em um índice usando a função hash e armazenando o valor no índice resultante no array subjacente.
166. Discussão sobre como a operação de buscar um valor dado uma chave em uma tabela hash envolve a transformação da chave em um índice usando a função hash e recuperando o valor no índice resultante no array subjacente.
167. Nota de que a operação de excluir um par chave-valor de uma tabela hash envolve a transformação da chave em um índice usando a função hash e removendo o valor no índice resultante no array subjacente.
168. Discussão sobre como a operação de inserir um novo par chave-valor, a operação de buscar um valor dado uma chave e a operação de excluir um par chave-valor de uma tabela hash são todas operações que são executadas em169. Discussão sobre como a busca por uma chave dada um valor em uma tabela hash levará tempo O(n).
170. Nota de que algumas pessoas muito inteligentes criaram funções de hash muito poderosas que podem ser usadas para minimizar o número de colisões.
171. Discussão sobre como as implementações populares de tabelas hash que existem no mundo real usam funções de hash muito sofisticadas e poderosas que minimizam o número de colisões.
172. Nota de que essas funções de hash que minimizam o número de colisões são tão poderosas que na indústria e em entrevistas de codificação, você normalmente esquece o cenário de pior caso O(n).
173. Discussão sobre como, em uma entrevista de codificação, se o entrevistador mencionar especificamente que você precisa lidar com casos extremos conhecidos e que você precisa de uma busca de tempo constante, então você precisaria se preocupar com o cenário de pior caso O(n) para tabelas hash.
174. Discussão sobre como, em entrevistas de codificação, você normalmente aceita que uma tabela hash suporta inserções, exclusões e buscas de tempo constante em média, mas você quase sempre trata isso como todo o tempo.
175. Discussão sobre como uma tabela hash na prática quase sempre parecerá ter valores separados em suas próprias listas vinculadas.
176. Nota de que, em entrevistas de codificação, você normalmente dá tanto poder às funções de hash que trata a ação de hash de chaves como uma operação de tempo constante.
177. Discussão sobre o que acontece quando você fica sem espaço no array subjacente de uma tabela hash, com a introdução do conceito de redimensionamento.
178. Discussão sobre como uma tabela hash é implementada usando um array subjacente, com cada índice do array armazenando um valor que é mapeado a partir de uma chave.
179. Explicação de que uma função hash é usada para transformar uma chave em um índice que se encaixa no array subjacente.
180. Discussão sobre como a função hash transforma uma chave, como uma string, em um número inteiro, mapeando cada caractere na string para seu valor de codificação ASCII e somando todos esses valores.
181. Nota de que o número resultante da função hash é então modulado pelo comprimento do array subjacente para garantir que o índice resultante se encaixe no array.
182. Discussão sobre como a operação de inserir um novo par chave-valor em uma tabela hash envolve a transformação da chave em um índice usando a função hash e armazenando o valor no índice resultante no array subjacente.
183. Discussão sobre como a operação de buscar um valor dado uma chave em uma tabela hash envolve a transformação da chave em um índice usando a função hash e recuperando o valor no índice resultante no array subjacente.
184. Nota de que a operação de excluir um par chave-valor de uma tabela hash envolve a transformação da chave em um índice usando a função hash e removendo o valor no índice resultante no array subjacente.
185. Discussão sobre como a operação de inserir um novo par chave-valor, a operação de buscar um valor dado uma chave e a operação de excluir um par chave-valor de uma tabela hash são todas operações que são executadas em tempo constante em média.
186. Discussão sobre como as árvores são uma estrutura de dados extremamente comum e poderosa, usada tanto na prática quanto em entrevistas de codificação.
187. Nota de que uma árvore é um tipo de grafo, mas no contexto de entrevistas de codificação, as árvores são bastante simples.
188. Discussão sobre como uma árvore é uma estrutura de grafo que é enraizada, significando que tem um nó raiz, ou conceitualmente, você pode pensar nisso como o nó superior da estrutura.
189. Discussão sobre como cada nó tem nós filhos, e a estrutura é direcionada, com as arestas apontando para baixo na árvore.
190. Nota de que a estrutura é acíclica, o que significa que não tem ciclos, e cada nó na árvore só pode ter um pai.
191. Discussão sobre como a árvore não pode ser desconectada, o que significa que você não pode ter, digamos, esta subárvore direita aqui desconectada e chamar esta estrutura inteira de árvore.
192. Discussão sobre como a maneira mais fácil de pensar em uma árvore é olhar para exemplos da vida real, como uma cadeia de gerenciamento em uma empresa onde você tem um gerente que tem relatórios diretos e esses relatórios diretos talvez sejam os gerentes de outras pessoas e assim por diante.
193. Introdução ao próximo tópico: Tipos de árvores.
194. Discussão sobre como existem muitos tipos diferentes de árvores, talvez o tipo mais comum de árvore seja o que chamamos de Árvore Binária, que é apenas uma árvore normal onde cada nó tem no máximo dois nós filhos.
195. Nota de que existem outros tipos de árvores, como a Árvore Ternária, onde cada nó pode ter até três nós filhos.
196. Discussão sobre como as árvores são uma estrutura de dados extremamente comum e poderosa, usada tanto na prática quanto em entrevistas de codificação.
197. Nota de que uma árvore é um tipo de grafo, mas no contexto de entrevistas de codificação, as árvores são bastante simples.
198. Discussão sobre como uma árvore é uma estrutura de grafo que é enraizada, significando que tem um nó raiz, ou conceitualmente, você pode pensar nisso como o nó superior da estrutura.
199. Discussão sobre como cada nó tem nós filhos, e a estrutura é direcionada, com as arestas apontando para baixo na árvore.
200. Nota de que a estrutura é acíclica, o que significa que não tem ciclos, e cada nó na árvore só pode ter um pai.
201. Discussão sobre como a árvore não pode ser desconectada, o que significa que você não pode ter, digamos, esta subárvore direita aqui desconectada e chamar esta estrutura inteira de árvore.
202. Discussão sobre como a maneira mais fácil de pensar em uma árvore é olhar para exemplos da vida real, como uma cadeia de gerenciamento em uma empresa onde você tem um gerente que tem relatórios diretos e esses relatórios diretos talvez sejam os gerentes de outras pessoas e assim por diante.
203. Introdução ao próximo tópico: Tipos de árvores.
204. Discussão sobre como existem muitos tipos diferentes de árvores, talvez o tipo mais comum de árvore seja o que chamamos de Árvore Binária, que é apenas uma árvore normal onde cada nó tem no máximo dois nós filhos.
205. Nota de que existem outros tipos de árvores, como a Árvore Ternária, onde cada nó pode ter até três nós filhos.
206. Discussão sobre como a busca por uma chave dada um valor em uma tabela hash levará tempo O(n).
207. Nota de que algumas pessoas muito inteligentes criaram funções de hash muito poderosas que podem ser usadas para minimizar o número de colisões.
208. Discussão sobre como as implementações populares de tabelas hash que existem no mundo real usam funções de hash muito sofisticadas e poderosas que minimizam o número de colisões.
209. Nota de que essas funções de hash que minimizam o número de colisões são tão poderosas que na indústria e em entrevistas de codificação, você normalmente esquece o cenário de pior caso O(n).
210. Discussão sobre como, em uma entrevista de codificação, se o entrevistador mencionar especificamente que você precisa lidar com casos extremos conhecidos e que você precisa de uma busca de tempo constante, então você precisaria se preocupar com o cenário de pior caso O(n) para tabelas hash.
211. Discussão sobre como, em entrevistas de codificação, você normalmente aceita que uma tabela hash suporta inserções, exclusões e buscas de tempo constante em média, mas você quase sempre trata isso como todo o tempo.
212. Discussão sobre como uma tabela hash na prática quase sempre parecerá ter valores separados em suas próprias listas vinculadas.
213. Nota de que, em entrevistas de codificação, você normalmente dá tanto poder às funções de hash que trata a ação de hash de chaves como uma operação de tempo constante.
214. Discussão sobre o que acontece quando você fica sem espaço no array subjacente de uma tabela hash, com a introdução do conceito de redimensionamento.
215. Discussão sobre como a complexidade de uma busca em uma árvore é logarítmica (log N) se a árvore estiver balanceada.
216. Nota de que no pior caso, a complexidade da busca seria linear (O(N)) se a árvore estivesse desbalanceada e formasse uma linha reta.
217. Introdução a alguns termos de vocabulário relacionados a árvores: um caminho em uma árvore que começa no nó raiz e termina em um dos nós inferiores é chamado de "ramo"; os nós inferiores são chamados de
218. Discussão sobre como a complexidade de uma busca em uma árvore é logarítmica (log N) se a árvore estiver balanceada.
219. Nota de que no pior caso, a complexidade da busca seria linear (O(N)) se a árvore estivesse desbalanceada e formasse uma linha reta.
220. Introdução a alguns termos de vocabulário relacionados a árvores: um caminho em uma árvore que começa no nó raiz e termina em um dos nós inferiores é chamado de "ramo"; os nós inferiores são chamados de "folhas".
221. Encerramento da aula e convite para a próxima aula.